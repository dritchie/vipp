
var nnutils = require('./utils');
var assert = require('assert');
var _ = require('underscore');
var numeric = require('numeric');
var tensor = require('../tensor');


// 'globals' contains the params object
function makeParamPredictor(opts) {

	var globals = opts.globals;
	var stateFeatures = opts.stateFeatures;
	var treeNodeFeatures = opts.treeNodeFeatures;
	var latentN = opts.latentN;
	var nNormalizeSamples = opts.nNormalizeSamples;


	// These get filled in at runtime
	var stateVecDim;
	var treeNodeDataDim;


	// Can't be passed in via opts b/c it needs to be AD'ed.
	// var activationFn = function(x) { return 1.7159 * Math.tanh(2/3 * x); };
	var activationFn = function(x) { return (2 / (1 + Math.exp(-x))) - 1; };


	var normCache = nnutils.makeInputSampleCache(nNormalizeSamples);
	function getNormalizedStateFeatures(state) {
		return nnutils.normalizeInputs('stateFeatures', stateFeatures(state), normCache);
	}
	function getNormalizedTreeNodeFeatures(node) {
		var features = treeNodeFeatures(node);
		if (features === undefined)
			return undefined;
		return nnutils.normalizeInputs('treeNodeFeatures', features, normCache);
	}


	var HYPERS = [0, 0.0001];
	function weightMatrix(name, dims) {
		return paramTensor(name, globals.params, dims, undefined, gaussianERP.sample, HYPERS);
	}
	function biasVector(name, length) {
		return paramTensor(name, globals.params, [length], undefined, gaussianERP.sample, HYPERS);
	}


	// Matrix fused multiply add
	function madd(A, x, b) {
		var n = b.length;
		var m = x.length;
		var out = b.slice();
		for (var i = 0; i < n; i++) {
			for (var j = 0; j < m; j++) {
				out[i] = out[i] + (A[i][j] * x[j]);
			}
		}
		return out;
	}

	// Neural net layer
	function nn(A, x, b) {
		return madd(A, x, b).map(activationFn);
	}


	var latentOrigin = numeric.rep([latentN], 0);
	function getLatentState(treeRoot) {
		if (treeRoot === undefined)
			return latentOrigin;
		var rnnParams = {
			// Params for nn that lifts tree node features into the latent space
			liftWeights: weightMatrix('liftWeights', [latentN, treeNodeDataDim]),
			liftBias: biasVector('liftBias', latentN),
			// Params for nn that merges two child latent states into one
			mergeWeights: weightMatrix('mergeWeights', [latentN, 2*latentN]),
			mergeBias: biasVector('mergeBias', latentN)
			// TODO: Separate params for incorporating lifted node features into merged children?
		};
		return _getLatentState(treeRoot, rnnParams);
	}
	// Subtree computations are cached
	var nodeIdCounter = 0;
	function nextNodeId() { return nodeIdCounter++; }
	function resetNodeIds() { nodeIdCounter = 0; }
	var latentStateCache = {};
	function clearLatentStateCache() { latentStateCache = {}; }
	function _getLatentState(node, rnnParams) {
		if (node.__RNN_ID === undefined) {
			node.__RNN_ID = nextNodeId();
			latentStateCache[node.__RNN_ID] = _getLatentStateImpl(node, rnnParams);
		}
		return latentStateCache[node.__RNN_ID];
	}
	function _getLatentStateImpl(node, rnnParams) {
		var features = getNormalizedTreeNodeFeatures(node);
		var latentState = nn(rnnParams.liftWeights, features, rnnParams.liftBias);
		if (node.children !== undefined && node.children.length > 0) {
			assert(node.children.length <= 2);	// Only support binary branching for now
			var childrenLatentState;
			if (node.children.length === 1) {
				childrenLatentState = _getLatentState(node.children[0], rnnParams);
			} else {
				childrenLatentState = nn(
					rnnParams.mergeWeights,
					_getLatentState(node.children[0], rnnParams).concat(_getLatentState(node.children[1], rnnParams)),
					rnnParams.mergeBias
				);
			}
			// TODO: Separate params for incorporating lifted node features into merged children?
			latentState = nn(rnnParams.mergeWeights, childrenLatentState.concat(latentState), rnnParams.mergeBias);
		}
		// ////
		// if (!tensor.all(latentState, function(x) { return isFinite(ad_primal(x)); })) {
		// 	console.log(latentState.map(ad_primal));
		// 	throw 'Non-finite latent state';
		// }
		// ////
		return latentState;
	}


	return {
		readyToPredict: function() {
			return normCache.hasEnoughSamples();
		},

		collectStateSample: function(state) {
			var features = stateFeatures(state);
			stateVecDim = features.length;
			nnutils.collectSample('stateFeatures', features, normCache);
		},

		collectTreeNodeSample: function(node) {
			var features = treeNodeFeatures(node);
			treeNodeDataDim = features.length;
			nnutils.collectSample('treeNodeFeatures', features, normCache);
		},

		registerNewTreeNode: function(node) {
			// Follow parent pointers back to the root and remove all ids
			while(node) {
				node.__RNN_ID = undefined;
				node = node.parent;
			}
		},

		prepareForRun: function() {
			resetNodeIds();
			clearLatentStateCache();
		},

		predict: function(name, origParams, bounds, currState, treeRoot) {

			var currLatentState = getLatentState(treeRoot);
			var predictWeights = weightMatrix('predictWeights_'+name, [bounds.length, stateVecDim + latentN]);
			var predictBias = biasVector('predictBias_'+name, bounds.length);
			var stateFeatures = getNormalizedStateFeatures(currState);
			var params = madd(predictWeights, stateFeatures.concat(currLatentState), predictBias);

			// var predictWeights = weightMatrix('predictWeights_'+name, [bounds.length, stateVecDim]);
			// var predictBias = biasVector('predictBias_'+name, bounds.length);
			// var stateFeatures = getNormalizedStateFeatures(currState);
			// var params = madd(predictWeights, stateFeatures, predictBias);

			// var params = biasVector('predictBias_'+name, bounds.length);

			var boundedParams = numeric.rep([params.length], 0);
			for (var i = 0; i < params.length; i++) {
				// The neural nets are set up so that when all the weights are zero, the output is zero.
				// With this in mind, we try to make params close to their original (i.e. as written in
				//    the program) values when all the weights are close to zero (i.e. at initialization).
				var offset = bounds[i].rvs(origParams[i]);
				boundedParams[i] = bounds[i].fwd(params[i] + offset);
			}
			// ////
			// // if (tensor.any(boundedParams, function(x) { return !isFinite(ad_primal(x)); })) {
			// if (tensor.any(boundedParams, function(x) { return Math.abs(ad_primal(x)) > 100; })) {
			// 	console.log('-----------');
			// 	console.log('currLatentState:');
			// 	console.log(currLatentState.map(ad_primal));
			// 	console.log('stateFeatures:');
			// 	console.log(stateFeatures.map(ad_primal));
			// 	console.log('predictWeights:');
			// 	console.log(tensor.map(predictWeights, function(x) { return ad_primal(x); }));
			// 	console.log('predictBias:');
			// 	console.log(predictBias.map(ad_primal));
			// 	console.log('pre-bounds predicted params:');
			// 	console.log(params.map(ad_primal));
			// 	console.log('final predicted params:');
			// 	console.log(boundedParams.map(ad_primal));
			// }
			// ////
			return boundedParams;
		}
	};
}


module.exports = {
	makeParamPredictor: makeParamPredictor
};




